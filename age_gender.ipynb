{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1e26608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\BrainAI\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from utils.opv import OpvModel\n",
    "mymodel1=OpvModel(\"face-detection-adas-0001\",device=\"CPU\",fp=\"FP32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6937adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "friends=cv2.imread(\"friends.jpeg\")\n",
    "friends=cv2.resize(friends,(600,400))\n",
    "predictions=mymodel1.Predict(friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd992af",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENDERS_FOR_OPENVINO=['Female','Male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7374b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawBoundingBoxes(predictions, image, conf=0.8):\n",
    "    mymodel2=OpvModel(\"age-gender-recognition-retail-0013\",device=\"CPU\",fp=\"FP32\")\n",
    "    \n",
    "    canvas=image.copy()\n",
    "    predictions_1=predictions[0][0] # numpy array 타입\n",
    "    confidence=predictions_1[:,2]   # 2번 데이터만\n",
    "    topresults=predictions_1[(confidence>conf)]  # conf 보다 큰 데이터만\n",
    "    (h,w)=canvas.shape[:2]\n",
    "    for detection in topresults:\n",
    "        box=detection[3:7]*np.array([w,h,w,h])\n",
    "        (xmin,ymin,xmax,ymax)=box.astype(\"int\")\n",
    "        img=canvas[ymin:ymax,xmin:xmax].copy()\n",
    "        img=cv2.resize(img,(62,62))\n",
    "        blob=cv2.dnn.blobFromImage(img,size=(62,62),ddepth=cv2.CV_8U)\n",
    "        openvino_net.setInput(blob)\n",
    "        detections=openvino_net.forwardAndRetrieve(['prob','age_conv3'])\n",
    "        print(detections)\n",
    "        gender=GENDERS_FOR_OPENVINO[detections[0][0][0].argmax()]\n",
    "        age=detections[1][0][0][0][0][0]*100\n",
    "#         predictions_age=mymodel2.Predict(img)\n",
    "        print(gender, age)\n",
    "#         print(predictions_age)\n",
    "        cv2.rectangle(canvas,(xmin,ymin),(xmax,ymax),(0,0,255),4)\n",
    "#         cv2.putText(canvas,emo+\" \"+str(round(max_emo_val*100,1))+\"%\",(xmin,ymin),cv2.FONT_HERSHEY_SIMPLEX,0.6,(255,0,0),2)\n",
    "    cv2.putText(canvas,str(len(topresults))+\" face(s) detected\",(50,50),cv2.FONT_HERSHEY_SIMPLEX,0.6,(255,0,0),2)\n",
    "    \n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"age-gender\",DrawBoundingBoxes(predictions,friends))\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ada56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import cv2\n",
    "ap=argparse.ArgumentParser()\n",
    "ap.add_argument(\"-x\",\"--xml\",required=True,help=\"path to the IR xml file\")\n",
    "ap.add_argument(\"-b\",\"--bin\",required=True,help=\"path to the IR bin file\")\n",
    "ap.add_argument(\"-p\",\"--path\",help=\"path to image files\")\n",
    "ap.add_argument(\"-t\",\"--target\",help=\"which device should be used for inference\")\n",
    "args=vars(ap.parse_args())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509cb2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENDERS_FOR_OPENVINO=['Female','Male']\n",
    "openvino_net1=cv2.dnn.readNet('./models/age-gender-recognition-retail-0013/FP32/age-gender-recognition-retail-0013.xml','./models/age-gender-recognition-retail-0013/FP32/age-gender-recognition-retail-0013.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5242501",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENDERS_FOR_OPENVINO=['Female','Male']\n",
    "\n",
    "def detect_age_and_gender_by_openvino(frame):\n",
    "    faces=face_cascade.detectMultiScale(\n",
    "    frame,\n",
    "    scaleFactor=1.2,\n",
    "    minNeighbors=6,\n",
    "    minSize=(30,30),\n",
    "    flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w),y+h,(0,255,0),2)\n",
    "        face=frame[y:y+h,x:x+w]\n",
    "        blob=cv2.dnn.blobFromImage(face,size=(62,62),ddepth=cv2.CV_8U)\n",
    "        openvino_net.setInput(blob)\n",
    "        detections=openvino_net.forwardAndRetrieve(['prob','age_conv3'])\n",
    "        gender=GENDERS_FOR_OPENVINO[detections[0][0][0].argmax()]\n",
    "        age=detections[1][0][0][0][0][0]*100\n",
    "        text=\"gender={}, age={:.0f}\".format(gender,age)\n",
    "        if gender=='Male':\n",
    "            color=(255,0,0)\n",
    "        else:\n",
    "            color=(0,0,255)\n",
    "        cv2.putText(frame,text,(10,20),cv2.FONT_HERSHEY_SIMPLEX,0.5,color,1)\n",
    "        return frame\n",
    "    \n",
    "    for file in glob.glob(args[\"path\"]+'/*'):\n",
    "        filename=str(file)\n",
    "        image=cv2.imread(filename)\n",
    "        openvino_image=detect_age_and_gender_by_openvino(image)\n",
    "        cv2.imshow(\"OpenVINO\",openvino_image)\n",
    "        cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57236d10",
   "metadata": {},
   "source": [
    "# DNN 모듈 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ceed3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "age_gender_model=cv2.dnn.readNet('./models/age-gender-recognition-retail-0013/FP32/age-gender-recognition-retail-0013.xml','./models/age-gender-recognition-retail-0013/FP32/age-gender-recognition-retail-0013.bin')\n",
    "face_model=cv2.dnn.readNet('./models/face-detection-adas-0001/FP32/face-detection-adas-0001.xml','./models/face-detection-adas-0001/FP32/face-detection-adas-0001.bin')\n",
    "GENDERS_FOR_OPENVINO=['F','M']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f38e8293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawBoundingBoxes(predictions, image, conf=0.8):    \n",
    "    canvas=image.copy()\n",
    "    predictions_1=predictions[0][0] # numpy array 타입\n",
    "    confidence=predictions_1[:,2]   # 2번 데이터만\n",
    "    topresults=predictions_1[(confidence>conf)]  # conf 보다 큰 데이터만\n",
    "    (h,w)=canvas.shape[:2]\n",
    "    for detection in topresults:\n",
    "        box=detection[3:7]*np.array([w,h,w,h])\n",
    "        (xmin,ymin,xmax,ymax)=box.astype(\"int\")\n",
    "        img=canvas[ymin:ymax,xmin:xmax].copy()\n",
    "        img=cv2.resize(img,(62,62))\n",
    "        \n",
    "        blob=cv2.dnn.blobFromImage(img,size=(62,62),ddepth=cv2.CV_8U)\n",
    "        age_gender_model.setInput(blob)\n",
    "        detections=age_gender_model.forwardAndRetrieve(['prob','age_conv3'])\n",
    "        \n",
    "        print(detections)\n",
    "        gender=GENDERS_FOR_OPENVINO[detections[0][0][0].argmax()]\n",
    "        age=detections[1][0][0][0][0][0]\n",
    "\n",
    "        cv2.rectangle(canvas,(xmin,ymin),(xmax,ymax),(0,0,255),4)\n",
    "        if gender=='F':\n",
    "            cv2.putText(canvas,gender+\" \"+str(round(age*100,1)),(xmin,ymin),cv2.FONT_HERSHEY_SIMPLEX,0.6,(203,192,255),2)\n",
    "        else:\n",
    "            cv2.putText(canvas,gender+\" \"+str(round(age*100,1)),(xmin,ymin),cv2.FONT_HERSHEY_SIMPLEX,0.6,(255,0,0),2)\n",
    "\n",
    "    cv2.putText(canvas,str(len(topresults))+\" face(s) detected\",(50,50),cv2.FONT_HERSHEY_SIMPLEX,0.6,(255,0,0),2)\n",
    "    \n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21804ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([[[[0.9133067 ]],\n",
      "\n",
      "        [[0.08669331]]]], dtype=float32)], [array([[[[0.21156797]]]], dtype=float32)]]\n",
      "[[array([[[[0.08485454]],\n",
      "\n",
      "        [[0.9151455 ]]]], dtype=float32)], [array([[[[0.3399426]]]], dtype=float32)]]\n",
      "[[array([[[[0.0034712]],\n",
      "\n",
      "        [[0.9965288]]]], dtype=float32)], [array([[[[0.23634905]]]], dtype=float32)]]\n",
      "[[array([[[[0.9323675 ]],\n",
      "\n",
      "        [[0.06763254]]]], dtype=float32)], [array([[[[0.20758618]]]], dtype=float32)]]\n"
     ]
    }
   ],
   "source": [
    "img=cv2.imread(\"images/friends.jpeg\")\n",
    "img=cv2.resize(img,(600,400))\n",
    "\n",
    "blob=cv2.dnn.blobFromImage(img)\n",
    "face_model.setInput(blob)\n",
    "predictions=face_model.forward()\n",
    "\n",
    "cv2.imshow(\"age-gender\",DrawBoundingBoxes(predictions,img))\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560494d0",
   "metadata": {},
   "source": [
    "### 동영상 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7e558c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.3.0-openvino-2020.3.0) C:\\jenkins\\workspace\\OpenCV\\OpenVINO\\2020.3\\build\\windows\\opencv\\modules\\dnn\\src\\ie_ngraph.cpp:522: error: (-215:Assertion failed) !isInitialized() in function 'cv::dnn::InfEngineNgraphNet::initPlugin'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a9b21aae38af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mblob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblobFromImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mface_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mpredictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mface_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Press Spacebar to Exit'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDrawBoundingBoxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m&\u001b[0m\u001b[1;36m0xFF\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.3.0-openvino-2020.3.0) C:\\jenkins\\workspace\\OpenCV\\OpenVINO\\2020.3\\build\\windows\\opencv\\modules\\dnn\\src\\ie_ngraph.cpp:522: error: (-215:Assertion failed) !isInitialized() in function 'cv::dnn::InfEngineNgraphNet::initPlugin'\n"
     ]
    }
   ],
   "source": [
    "camera=cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret,frame=camera.read()\n",
    "    if not(ret):\n",
    "        break\n",
    "    blob=cv2.dnn.blobFromImage(frame)\n",
    "    face_model.setInput(blob)\n",
    "    predictions=face_model.forward()\n",
    "    cv2.imshow('Press Spacebar to Exit',DrawBoundingBoxes(predictions,frame))\n",
    "    if cv2.waitKey(1)&0xFF == ord(' '):\n",
    "        break\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a51baa",
   "metadata": {},
   "source": [
    "# Open CV DNN 모듈\n",
    "### - 미리 학습된 딥러닝 모델을 이용하여 실행(forward pass, inference)하는 기능\n",
    "### - 학습은 지원하지 않음\n",
    "### - OpenCV 3.3 버전부터 기본 기능으로 제공\n",
    "### - OpenCV 4.3 버전부터 GPU(CUDA) 지원(소스 코드 직접 빌드 필요)\n",
    "### - 참고 : https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV\n",
    "### \n",
    "## 1) 네트워크 불러오기\n",
    "### * cv2.dnn.readNet(model, config=None, framework=None) -> retval\n",
    "### - model : 훈련된 가중치를 저장하고 있는 이진 파일 이름\n",
    "### - config : 네트워크 구성을 저장하고 있는 텍스트 파일 이름\n",
    "### - framework : 명시적인 딥러닝 프레임워크 이름\n",
    "### - retval : cv2.dnn_Net클래스 객체\n",
    "### \n",
    "## 2) 네트워크 입력 블롭(blob) 만들기\n",
    "### * cv2.dnn.blobFromImage(image, scalefactor=None, size=None, mean=None, \n",
    "###             swapRB=None, crop=None, ddepth=None) -> retval\n",
    "### - image : 입력 영상\n",
    "### - scalefactor : 입력 영상 픽셀 값에 곱할 값, 기본값은 1. 딥러닝 학습할 때 영상데이터를 픽셀 0~255로 볼 것이냐, 0~1까지 노멀라이즈로 해서 실수값으로 볼것이냐, 0~1이면 1/255를 주고, 1~255이면 1을 주면 된다. 모델파일이 어떤 식으로 학습되었는지 알아야 한다.\n",
    "### - size : 출력 영상의 크기, 기본값은 (0,0)\n",
    "### - mean : 입력 영상 각 채널에서 뺄 평균 값, 기본값은 (0,0,0,0). 내가 사용할 데이터셋의 평균\n",
    "### - swapRB : R과 B 채널을 서로 바꿀 것인지를 결정하는 플래그, 기본값은 False. BGR순서로 되어 있으므로 RGB로 바꿀 필요가 있을 때 결정하는 플래그\n",
    "### - crop : 크롭 수행 여부, 기본값은 False\n",
    "### - ddepth : 출력 블롭의 깊이, cv_32F 또는 CV_8U, 기본값은 CV_32F\n",
    "### - retval : 영상으로부터 구한 블롭 객체\n",
    "### numpy.ndarray.shape=(N,C,H,W).dtype=numpy.float32\n",
    "#### DNN객체의 입력 - 4차원형태의 행렬형식의 입력\n",
    "#### => N : 사진 개수, C : 채널개수 그레이는 2, H,W : 세로와 가로\n",
    "### \n",
    "## 3) 네트워크 입력 설정하기\n",
    "### * cv2.dnn_Net.setInput(blob, name=None, scalefactor=None, mean=None) -> None\n",
    "### - blob : 블롭 객체\n",
    "### - name : 입력 레이어 이름\n",
    "### - scalefactor : 추가적으로 픽셀 값에 곱할 값\n",
    "### - mean : 추가적으로 픽셀 값에서 뺄 평균 값\n",
    "### \n",
    "## 4) 네트워크 순방향 실행(추론)\n",
    "### * cv2.dnn_Net.forward(outputName=None) -> retval\n",
    "### * cv2.dnn_Net.forward(outputNames=None, outputBlobs=None) -> outputBlobs\n",
    "### - outputName : 출력 레이어 이름\n",
    "### - retval : 지정한 레이어의 출력 블롭, 네트워크마다 다르게 결정됨\n",
    "### - outputNames : 출력 레이어 이름 리스트\n",
    "### - outputBlobs : 지정한 레이어의 출력 블롭 리스트\n",
    "### \n",
    "### 아래는 dnn 모듈을 이용하여 숫자 인식 예제\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec215f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "oldx, oldy = -1,-1\n",
    "\n",
    "def on_mouse(event, x, y, flags, _):\n",
    "    global oldx, oldy\n",
    "    \n",
    "    if event==cv2.EVENT_LBUTTONDOWN:\n",
    "        oldx, oldy= x, y\n",
    "    elif event==cv2.EVENT_LBUTTONUP:\n",
    "        oldx, oldy= -1, -1\n",
    "    elif event==cv2.EVENT_MOUSEMOVE:\n",
    "        if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "            cv2.line(img,(oldx, oldy),(x,y),(255,255,255),40,cv2.LINE_AA)\n",
    "            oldx,oldy=x,y\n",
    "            cv2.imshow('img',img)\n",
    "\n",
    "def norm_digit(img):\n",
    "    m=cv2.moments(img)\n",
    "    cx=m['m10']/m['m00']\n",
    "    cy=m['m01']/m['m00']\n",
    "    h,w=img.shape[:2]\n",
    "    aff=np.array([[1,0,w/2-cx],[0,1,h/2-cy]], dtype=np.float32)\n",
    "    return dst\n",
    "    \n",
    "net=cv2.dnn.readNet('mnist_cnn.pb')  # mnist_cnn.pb 파일로부터 네트워크 생성\n",
    "\n",
    "if net.empty():\n",
    "    print('Network load failed')\n",
    "    sys.exit()\n",
    "    \n",
    "img=np.zeros((400,400), np.uint8)\n",
    "cv2.imshow('img',img)\n",
    "cv2.setMouseCallback('img',on_mouse)\n",
    "\n",
    "while True :\n",
    "    c=cv2.waitKey()\n",
    "    if c==27:\n",
    "        break\n",
    "    elif c==ord(' '):\n",
    "        blob=cv2.dnn.blobFromImage(norm_digit(img), 1/255., (28,28)) # blob : shape=(1,1,28,28). dtype=float32\n",
    "        net.setInput(blob)                               \n",
    "        prob=net.forward()                               # prob : shape=(1,10). dtype=float32\n",
    "        \n",
    "        _, maxVal, _, maxLoc = cv2.minMaxLoc(prob)       # prob 행렬에서 최대값 위치가 인식된 숫자를 나타냄\n",
    "        digit=maxLoc[0]                                 \n",
    "        \n",
    "        print(f'{digit} ({maxVal*100:4.2f}%)')\n",
    "        img.fill(0)\n",
    "        cv2.imshow('img',img)\n",
    "cv2.destroyAllWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f685ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
